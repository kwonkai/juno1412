{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKO3PtJzhcp0"
      },
      "source": [
        "#Neural Network Embedding Recommendation System\n",
        "1. Load in data and clean\n",
        "2. Prepare data for supervised machine learning task\n",
        "3. Build the entity embedding neural network\n",
        "4. Train the neural network on prediction task\n",
        "5. Extract embeddings and find most similar books and wikilinks\n",
        "6. Visualize the embeddings using dimension reduction techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz5USNEwgQuW"
      },
      "source": [
        "## 1. Read Data & Clean\n",
        ": 모든 책의 데이터는 json으로 저장되어 있음. 해당 데이터는 위키피디아의 모든 책에 대한 기사가 포함되어 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7AZUH2mpK7p"
      },
      "source": [
        "1-1. 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy2s0AvFe3wn"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "# Set shell to show all lines of output\n",
        "# jupyter notebook에서 모든 ourput 나타내기\n",
        "InteractiveShell.ast_node_interactivity = 'all'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qCAPQXSndSQ"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# print(os.listdir(\"./input\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPxHGX5SjObw",
        "outputId": "4c453472-48c8-49f3-bb59-48c65efd12ac"
      },
      "outputs": [],
      "source": [
        "# utils import get_file == url -> file download\n",
        "import tensorflow as tf\n",
        "# from keras.utils import get_file\n",
        "\n",
        "x = tf.keras.utils.get_file('found_books_filtered.ndjson', 'https://raw.githubusercontent.com/WillKoehrsen/wikipedia-data-science/master/data/found_books_filtered.ndjson')\n",
        "\n",
        "import json\n",
        "\n",
        "books = []\n",
        "\n",
        "with open(x, 'r') as fin: # 'r' 읽기용으로 파일 열기\n",
        "    # Append each line to the books\n",
        "    books = [json.loads(l) for l in fin]\n",
        "\n",
        "# Remove non-book articles\n",
        "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
        "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
        "print(f'Found {len(books)} books.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3mghEVTpHnr"
      },
      "source": [
        "1-2. 데이터 전처리\n",
        "- 책에 관한 모든 페이지를 검색해 책의 제목, 기본 정보, 다른 위키피디아 페이지(위키링크)를 가리키는 링크, 외부 사이트 링크를 저장\n",
        "- 추천 시스템을 만들기 위해 필요한 정보는 제목과 위키링크 두 가지입니다\n",
        "- 일부 기사에는 책에대한 기사가 아닌 것들을 잡아낸다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdO5-yTyo1Nu",
        "outputId": "9e9a8117-0d8c-4137-c534-c70649e7059f"
      },
      "outputs": [],
      "source": [
        "# book list 내부의 book_with_wikipedia 일부 가져오기\n",
        "[book[0] for book in books_with_wikipedia][:3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZegunCLxAFa",
        "outputId": "33cec001-e6f5-4bca-c3ea-5ae77c22f42d"
      },
      "outputs": [],
      "source": [
        "# 제목, 'infoboxs book'의 정보, 위키피디아 링크, 외부링크, 최종수정날짜, 기사의 문자 수\n",
        "# title, information from 'infobos book' template, wikipedia links, externel links, the date of last edit, the number of characters in article\n",
        "n = 21\n",
        "books[n][0], books[n][1], books[n][2][:5], books[n][:5], books[n][:5], books[n][4], books[n][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EkWSE3QTp-Of",
        "outputId": "425cb5b9-27ef-4969-d6a7-6a040175517f"
      },
      "outputs": [],
      "source": [
        "# 책정보 정수로 변경하기 # index \n",
        "book_index = {book[0] : idx for idx, book in enumerate(books)} #enumerate : 인덱스, 원소로 이루어진 tuple로 만들어줌\n",
        "index_book = {idx : book for book, idx in book_index.items()} # items() : key와 대응값 가져오기 # book_index의 대응값 'title' 가져오기\n",
        "\n",
        "book_index['Dreaming Spies']\n",
        "index_book[98]\n",
        "index_book[100]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oOBE_36uXIJ",
        "outputId": "864038b5-f548-4e80-a55c-4c9899c52131"
      },
      "outputs": [],
      "source": [
        "# Exploring Wikilinks\n",
        "# chain method = 자기자신을 반환하면서 다른 함수를 지속적으로 호출할 수 있는 방법\n",
        "from itertools import chain\n",
        "wikilinks = list(chain(*[book[2] for book in books]))\n",
        "print(f\"There are {len(set(wikilinks))} unique wikilinks.\") # set() 중복제거\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clvBv9hbxhZ_",
        "outputId": "903e552b-e075-4ff7-9ce9-298e86708120"
      },
      "outputs": [],
      "source": [
        "# 다른책에는 얼마나 많은 wiki link가 있나?\n",
        "wikilinks_other_book = [link for link in wikilinks if link in book_index.keys()] #link에 key, 대응값이 있다면 wikilinks에서 link list로 뽑아 만든다\n",
        "print(f\"There are {len(set(wikilinks_other_book))} unique wikilinks to other books\") # 중복치 제거 길이값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iILdbuzIyOJy"
      },
      "outputs": [],
      "source": [
        "# 가장 많이 연결된 기사 찾기\n",
        "# items 항목 수가 카운트된 dictionary를 반환하는 함수를 만든다.\n",
        "# collections module : count(개수세기), OrderedDict\n",
        "\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def count_items(l):\n",
        "  # Return ordered dictionary of counts of objects in `l`\n",
        "  # create a count object\n",
        "  counts = Counter(l)\n",
        "\n",
        "  # sort by highest count first and place in orderd dictionary\n",
        "  # sort(key = (key인자에 함수를 넘겨주면 우선순위가 정해진다))\n",
        "  counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)  # x[1] 우선순위 숫자로 변경, reverse = 높은 숫자부터\n",
        "  counts = OrderedDict(counts) # 데이터 순서 설정(key, val)\n",
        "\n",
        "  return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujQHR6VY3dke",
        "outputId": "b37942e1-8e35-418f-9168-39240ad62ead"
      },
      "outputs": [],
      "source": [
        "# Find set of wikilinks from each book and convert to a flattend last\n",
        "# 각각 책에서 wikilinks 설정을 찾고 1차원으로 변경하기\n",
        "\n",
        "# list(chain(*(set ~~))) = ????\n",
        "\n",
        "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books])) # books의 중복치를 제거한 wikilinks 값\n",
        "\n",
        "wikilink_counts = count_items(unique_wikilinks) # 가장 많이 사용된 wikilinks의 unique_counts 상위 10개 불러오기\n",
        "list(wikilink_counts.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90DDQwhWU_w6",
        "outputId": "571c17c4-5263-4eb1-c1da-336889804f53"
      },
      "outputs": [],
      "source": [
        "# 대문자 -> 소문자로 변경하기\n",
        "wikilinks = [link.lower() for link in unique_wikilinks] # lower() 대문자 -> 소문자 : 동일링크 : paperback, Paperback, PAPERBACK 등 링크 통합\n",
        "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
        "\n",
        "wikilink_counts = count_items(wikilinks)\n",
        "list(wikilink_counts.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "cPUOeKURbpSC",
        "outputId": "b8818f38-70df-4717-cc3b-ae824956ccee"
      },
      "outputs": [],
      "source": [
        "# 데이터 시각화\n",
        "# wikilink_ count_top10\n",
        "\n",
        "# for i in range(11):\n",
        "#   wikilink_counts_top = list(wikilink_counts.items())[i]\n",
        "import matplotlib.pyplot as plt\n",
        "wikilink_counts_top = list(wikilink_counts.items())[:10]\n",
        "\n",
        "index = [8740, 8648, 6043, 6016, 5665, 4248, 3063, 2983, 2742, 2003]\n",
        "columns = ['paperback', 'hardcover', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels', 'science fiction', 'english language', 'united states', 'novel', 'the new york times', 'fantasy']\n",
        "bar_plot = plt.barh(columns, index)\n",
        "\n",
        "# def autolabel(rects):\n",
        "#     for idx,rect in enumerate(bar_plot):\n",
        "#         height = rect.get_height()\n",
        "#         ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,index[idx],\n",
        "#                 ha='center', va='bottom', rotation=0)\n",
        "# autolabel(bar_plot)\n",
        "plt.title('wikilink_counts_top10', fontsize=20)\n",
        "plt.xlabel = ('unique wikilinks')\n",
        "plt.ylabel = ('counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf7vMVkE1KaY"
      },
      "outputs": [],
      "source": [
        "## wikilinks unique값 구하기\n",
        "# 가장 많은 wikilink 제거하기\n",
        "# paperback, hardcover, wikipedia:wikiproject books, wikipedia:wikiproject novels\n",
        "# 이유 \n",
        "# 1) paperback(얇은 가벼운 재질의 책), hardcover(딱딱한 겉표지), hardback(=hardcover), e-book(책의 종류, 내용 X) -> 도서정보와 관련이 없음\n",
        "# 2) wikiproject books, wikiproject novels (wikiproject = 단순 책에 대한 정보 정리, 기사 X, 콘텐츠 기반 X)\n",
        "\n",
        "to_remove = ['hardcover', 'paperback', 'hardback', 'e-book', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels'] \n",
        "\n",
        "for t in to_remove:\n",
        "    wikilinks.remove(t)\n",
        "    _ = wikilink_counts.pop(t) # ????? #pop(t) t가 들어간 to_move의 카테고리들을 제거해라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu-3SnO10Jpb",
        "outputId": "d08d9042-254b-4a44-e230-700458428e4f"
      },
      "outputs": [],
      "source": [
        "# 4번 이상 나온 wikilinks를 사용한다.\n",
        "\n",
        "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4] # ?????\n",
        "type(links)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7VKwcsP1ypm",
        "outputId": "915c2f05-dad3-4228-afa1-9006d8bf794c"
      },
      "outputs": [],
      "source": [
        "# wikipedia에서 다른 책과 가장 많이 연결된 도서 top10\n",
        "# 각 책에대한 book wikilinks 찾기\n",
        "unique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books])) # * 모든것 : chain(10) -> 10 11 12 13 14 이어붙이기 \n",
        "\n",
        "# 다른책에서 링크된 책의 숫자\n",
        "wikilinks_book_counts = count_items(unique_wikilinks_books)\n",
        "list(wikilinks_book_counts.items())[:10]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "LVwpVViBl_kH",
        "outputId": "aca005bc-43d9-4375-e454-7a5daa2f4541"
      },
      "outputs": [],
      "source": [
        "index = [127, 104, 63, 55, 51, 51, 49, 49, 47, 39]\n",
        "columns = ['The Encyclopedia of Science Fiction', 'The Discontinuity Guide', 'The Encyclopedia of Fantasy', 'Dracula', 'Encyclopædia Britannica', 'Nineteen Eighty-Four', 'Don Quixote', 'The Wonderful Wizard of Oz', \"Alice's Adventures in Wonderland\", 'Jane Eyre']\n",
        "bar_plot = plt.barh(columns, index)\n",
        "\n",
        "plt.title('Most linked to books by Wikipedia books', fontsize=20)\n",
        "plt.xlabel = ('unique wikilinks')\n",
        "plt.ylabel = ('linked counts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IES_6zwGnuEJ",
        "outputId": "51e8b7fb-1ef6-4acc-f4f9-d4813ef33e35"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리 결과\n",
        "print(f'Found {len(books)} books.')\n",
        "print(f'Found {len(links)} links.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4HuG2ulm_QJ"
      },
      "source": [
        "##추가 전처리 작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxecViCv3id-",
        "outputId": "39e30592-ca4f-4c6a-cfe9-34289bcaf804"
      },
      "outputs": [],
      "source": [
        "# 잠재적인 추가제거 작업\n",
        "# 데이터 전처리를 추가적으로 진행하고 싶다면 수행\n",
        "for book in books:\n",
        "    if 'The New York Times' in book[2] and 'New York Times' in book[2]:\n",
        "        print(book[0], book[2])\n",
        "        break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arOSYc-lDRER",
        "outputId": "23d8d8f8-4d84-4c01-e0e8-be53b3d7b126"
      },
      "outputs": [],
      "source": [
        "wikilink_counts.get('the new york times')\n",
        "\n",
        "wikilink_counts.get('new york times')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9wqMk2r0DjJV",
        "outputId": "7930c04e-8835-49c6-9fca-7f937e8f6cc3"
      },
      "outputs": [],
      "source": [
        "# Wikilinks to Index\n",
        "# book 데이터를 정수로 바꾸어주었듯이, Wikilinks도 정수로 바꿔준다\n",
        "link_index = {link: idx for idx, link in enumerate(links)}\n",
        "index_link = {idx: link for link, idx in link_index.items()}\n",
        "\n",
        "link_index['the economist']\n",
        "index_link[300]\n",
        "print(f'There are {len(link_index)} wikilinks that will be used.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A40a0D0gnzLt"
      },
      "source": [
        "######################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7H4jD3tnwQP"
      },
      "source": [
        "#2.Superised Machine Learning Task\n",
        "임베딩 신경망을 훈련하기 위한 머신 러닝 작업을 개발하기\n",
        "\n",
        "##  Build a Training Set\n",
        "지도학습 :\n",
        "(book, links)의 값이 주어지면 데이터에 있는 정보인지 예측하는 학습모델을 만든다.\n",
        "trainset을 만들기 위해 모든책의 title, wikilink는 (title, wikilink)튜플 형태로 저장한다. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT1HtQVmq0Sb"
      },
      "outputs": [],
      "source": [
        "# # 책 이름과 책의 고유 인덱스 index 맵핑\n",
        "# type(books)\n",
        "# book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
        "# print(book_index)\n",
        "\n",
        "# # 링크와 링크 고유 인덱스 mapping\n",
        "# # links = tuple(links)\n",
        "# type(links)\n",
        "# link_index = {book[2]: idx for idx, book in enumerate(links)}\n",
        "# print(link_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YCMae2mqGQ1"
      },
      "outputs": [],
      "source": [
        "# pairs = []\n",
        "\n",
        "# # 각각 책이 나오도록 반복\n",
        "# for book in books:\n",
        "\n",
        "#     title = book[0]\n",
        "#     book_links = book[2]\n",
        "#     # 책에 관한 글에 있는 wikilinks 들을 반복\n",
        "#     for link in book_links:\n",
        "#         # 책의 인덱스와 링크 페어 저장\n",
        "#         pairs.extend(book_index[title], link_index[link])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvL9rHDNo668",
        "outputId": "2aebea63-7ee6-441a-815d-3c44fed1bd0c"
      },
      "outputs": [],
      "source": [
        "pairs = [] # pairs 빈 list 생성\n",
        "\n",
        "# 각 책에대한 반복 수행\n",
        "for book in books:\n",
        "    # 각 책에대한 링크를 반복 수행\n",
        "    # 770,000개의 예시 추가\n",
        "    # 예시 각 title마다 link가 들어간 pairs만들기 (2, 616), (2, 2914) -> 77만개\n",
        "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n",
        "\n",
        "# 모델을 훈련시키기 위한 777,000개의 예시를 보여준다.\n",
        "len(pairs), len(links), len(books)\n",
        "pairs[5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhsS_r0C5vMl",
        "outputId": "422775ad-255b-404c-ad57-dee2b1b0d9b7"
      },
      "outputs": [],
      "source": [
        "# 모델을 훈련시키기 위한 777,000개의 예시를 보여준다.\n",
        "pairs[5000]\n",
        "\n",
        "pairs[50]\n",
        "pairs[51]\n",
        "pairs[52]\n",
        "pairs[53]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOKS3RplwlbE",
        "outputId": "e674f94c-60c2-4cc4-d8cd-02c7fa72a369"
      },
      "outputs": [],
      "source": [
        "# 777,000여개의 데이터 확인하기\n",
        "# 5000번대\n",
        "index_book[pairs[5000][0]], index_link[pairs[5000][1]]\n",
        "\n",
        "# 1200번대\n",
        "index_book[pairs[1200][0]], index_link[pairs[1200][1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uJ2vGXMxo1c",
        "outputId": "8f7b550a-aafb-4b9b-edad-cc87893f4795"
      },
      "outputs": [],
      "source": [
        "# 링크, 책 무작위 샘플링 후 확인 -> 맞지 않는 예시 만들기\n",
        "pairs_set = set(pairs)\n",
        "\n",
        "# 가장 자주 나타나는 (title, link)\n",
        "x = Counter(pairs)\n",
        "sorted(x.items(), key = lambda x: x[1], reverse = True)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Y5J8uy0L9g"
      },
      "source": [
        "## Train/Test set에 관한 참고사항\n",
        "validation set or testset을 만들지 않는데, accuracy를 측정하는 모델이 아니라 embedding model을 생성하는 게 주된 목표이다.\n",
        "\n",
        "model train 후, 새로운 데이터에 대한 model test가 없으므로 과적합을 방지할 필요가 없다. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivd3d6ai0LD8"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 positive, negative 생성기 만들기\n",
        "# 다시 복습 # 모르겠음 ㅠㅠ\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
        "  # batch를 저장할 numpy 배열 준비하기\n",
        "  batch_size = n_positive * (1 + negative_ratio)\n",
        "  batch = np.zeros((batch_size, 3)) # shape = batch_size * 3\n",
        "\n",
        "  # 라벨 조정하기\n",
        "  if classification:\n",
        "    neg_label = 0\n",
        "  else:\n",
        "    neg_label = -1\n",
        "\n",
        "  # 생성기 만들기\n",
        "  while True:\n",
        "    # 랜덤 positive 예시 선택\n",
        "    for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
        "      batch[idx, :] = (book_id, link_id, 1)\n",
        "\n",
        "    # idx = 1씩 증가\n",
        "    idx += 1\n",
        "\n",
        "    # batchsize가 찰때까지, negative examples 추가\n",
        "    while idx < batch_size:\n",
        "\n",
        "      # 랜덤선택\n",
        "      random_book = random.randrange(len(books))\n",
        "      random_link = random.randrange(len(links))\n",
        "\n",
        "      # positive sample이 아니라는 걸 체크\n",
        "      if (random_book, random_link) not in pairs_set:\n",
        "\n",
        "        # 배치에 negative_index  추가하기 \n",
        "        batch[idx, :] = (random_book, random_link, neg_label)\n",
        "        idx += 1\n",
        "\n",
        "      \n",
        "    # Make sure to shuffle order\n",
        "        np.random.shuffle(batch)\n",
        "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es6P7i1j_m8X",
        "outputId": "428ac32a-8ef7-43a2-cc66-0350b3096213"
      },
      "outputs": [],
      "source": [
        "# 새로운 batch 얻기\n",
        "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZrKccrbALnH",
        "outputId": "3d92fd5e-9571-431a-ca3f-738dd7367cd1"
      },
      "outputs": [],
      "source": [
        "# train pairs 예시 확인하기\n",
        "a, b = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
        "\n",
        "for label, book_idx, link_idx in zip(b, a['book'], a['link']):\n",
        "  print(f'Book: {index_book[book_idx]:30} Link : {index_link[link_idx]:40} Label : {label}')\n",
        "\n",
        "# x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
        "\n",
        "# for label, b_idx, l_idx in zip(y, x['book'], x['link']):\n",
        "#     print(f'Book: {index_book[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXgx1SCVU0b9"
      },
      "source": [
        "#3.Neural Network Embedding Model\n",
        "###5 layers \n",
        " 1) input layer : book, link에 대한 병렬 입력 \\\n",
        " 2) Embedding : book, link를 위한 병렬 50개 Embedding \\\n",
        " 3) Dot : 내적(Dot product)를 계산해 Embedding 합치기 \\\n",
        " 4) Reshape : Embedding shape를 단일 숫자로 형성 \\\n",
        " 5) Dense : sigmoid activation을 이용한 출력 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaTp6y8SUzOF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeXC8rPMbFG2",
        "outputId": "4b6aea16-9a50-487b-bcf4-75d27b6eff3a"
      },
      "outputs": [],
      "source": [
        "def book_embedding_model(embedding_size=50, classification = False):\n",
        "\n",
        "  # \"\"\"Model to embed books and wikilinks using the functional API.\n",
        "  #    \\Trained to discern if a link is present in a article\"\"\"\n",
        "\n",
        "    # 1차원 입력\n",
        "    book = Input(name='book', shape=[1])\n",
        "    link = Input(name='link', shape=[1])\n",
        "\n",
        "    # 책 Embedding(None, 1, 50)\n",
        "    book_embedding = Embedding(name = 'book_embedding',\n",
        "                               input_dim = len(book_index),\n",
        "                               output_dim = embedding_size)(book)\n",
        "\n",
        "    # link Embedding(None, 1, 50)\n",
        "    link_embedding = Embedding(name = 'link_embedding',\n",
        "                               input_dim = len(link_index),\n",
        "                               output_dim = embedding_size)(link)\n",
        "\n",
        "    # 내적으로 book&link embedding 1개의 Embedding으로 변형\n",
        "    # shape will be(None, 1, 1)\n",
        "    # Dot(name, normalize(정규화), axes(샘플 간 내적계산))\n",
        "    merged = Dot(name = 'dot_product', normalize = True, axes=2)([book_embedding, link_embedding])\n",
        "\n",
        "    # Reshape to be single Number(shape will be(None, 1))\n",
        "    merged = Reshape(target_shape = [1])(merged)\n",
        "\n",
        "    # if classifcation, add extra layers and loss function is binary crossentroy\n",
        "    if classification:\n",
        "        merged = Dense(1, activation = 'sigmoid')(merged)\n",
        "        model = Model(inputs = [book, link], outputs = merged)\n",
        "        model.compile(optimizer = 'Adam', loss = 'binary_crossentrypy', metrics = ['acccuracy'])\n",
        "\n",
        "      # Otherwise loss function is mean squared error\n",
        "    else:\n",
        "      # model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        model = Model(inputs = [book, link], outputs = merged)\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantitate model and show parameters\n",
        "model = book_embedding_model()\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqTGUCZ2mk9n"
      },
      "source": [
        "#4. TRAIN MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_positive = 1024\n",
        "\n",
        "gen = generate_batch(pairs, n_positive, negative_ratio=2)\n",
        "\n",
        "# Train\n",
        "# steps_per_epoch = 1epoch마다 사용할 batch_size를 정의함\n",
        "# verbose(상세정보) 보통 0, 자세히 1, 함축정보 2\n",
        "model.fit_generator(gen, epochs = 15, steps_per_epoch = len(pairs) // n_positive, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('first_attempt.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.Extract Embeddings and Analyze\n",
        "trainset은 Embedding 공간에서 similar entity를 옆에 배치하는 (title, wikilinks)를 학습했다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract embeddings\n",
        "book_layer = model.get_layer('book_embedding')\n",
        "book_weights = book_layer.get_weights()[0]\n",
        "book_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))\n",
        "book_weights[0][:10]\n",
        "np.sum(np.square(book_weights[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rcParams['font.size'] = 15\n",
        "\n",
        "def find_similar(name, weights, index_name = 'book', n = 10, least = False, return_dist = False, plot = False):\n",
        "    \"\"\"Find n most similar items (or least) to name based on embeddings. Option to also plot the results\"\"\"\n",
        "    \n",
        "    # Select index and reverse index\n",
        "    if index_name == 'book':\n",
        "        index = book_index\n",
        "        rindex = index_book\n",
        "    elif index_name == 'page':\n",
        "        index = link_index\n",
        "        rindex = index_link\n",
        "    \n",
        "    # Check to make sure `name` is in index\n",
        "    try:\n",
        "        # Calculate dot product between book and all others\n",
        "        dists = np.dot(weights, weights[index[name]])\n",
        "    except KeyError:\n",
        "        print(f'{name} Not Found.')\n",
        "        return\n",
        "    \n",
        "    # Sort distance indexes from smallest to largest\n",
        "    sorted_dists = np.argsort(dists)\n",
        "    \n",
        "    # Plot results if specified\n",
        "    if plot:\n",
        "        \n",
        "        # Find furthest and closest items\n",
        "        furthest = sorted_dists[:(n // 2)]\n",
        "        closest = sorted_dists[-n-1: len(dists) - 1]\n",
        "        items = [rindex[c] for c in furthest]\n",
        "        items.extend(rindex[c] for c in closest)\n",
        "        \n",
        "        # Find furthest and closets distances\n",
        "        distances = [dists[c] for c in furthest]\n",
        "        distances.extend(dists[c] for c in closest)\n",
        "        \n",
        "        colors = ['r' for _ in range(n //2)]\n",
        "        colors.extend('g' for _ in range(n))\n",
        "        \n",
        "        data = pd.DataFrame({'distance': distances}, index = items)\n",
        "        \n",
        "        # Horizontal bar chart\n",
        "        data['distance'].plot.barh(color = colors, figsize = (10, 8),edgecolor = 'k', linewidth = 2)\n",
        "        plt.xlabel('Cosine Similarity');\n",
        "        plt.axvline(x = 0, color = 'k');\n",
        "        \n",
        "        # Formatting for italicized title\n",
        "        name_str = f'{index_name.capitalize()}s Most and Least Similar to'\n",
        "        for word in name.split():\n",
        "            # Title uses latex for italize\n",
        "            name_str += ' $\\it{' + word + '}$'\n",
        "        plt.title(name_str, x = 0.2, size = 28, y = 1.05)\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    # If specified, find the least similar\n",
        "    if least:\n",
        "        # Take the first n from sorted distances\n",
        "        closest = sorted_dists[:n]\n",
        "         \n",
        "        print(f'{index_name.capitalize()}s furthest from {name}.\\n')\n",
        "        \n",
        "    # Otherwise find the most similar\n",
        "    else:\n",
        "        # Take the last n sorted distances\n",
        "        closest = sorted_dists[-n:]\n",
        "        \n",
        "        # Need distances later on\n",
        "        if return_dist:\n",
        "            return dists, closest\n",
        "        \n",
        "        \n",
        "        print(f'{index_name.capitalize()}s closest to {name}.\\n')\n",
        "        \n",
        "    # Need distances later on\n",
        "    if return_dist:\n",
        "        return dists, closest\n",
        "    \n",
        "    \n",
        "    # Print formatting\n",
        "    max_width = max([len(rindex[c]) for c in closest])\n",
        "    \n",
        "    # Print the most similar and distances\n",
        "    for c in reversed(closest):\n",
        "        print(f'{index_name.capitalize()}: {rindex[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('War and Peace', book_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('War and Peace', book_weights, n = 5, plot = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('The Fellowship of the Ring', book_weights, n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('Artificial Intelligence: A Modern Approach', book_weights, n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('Bully for Brontosaurus', book_weights, n = 5, plot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wikilink Embeddings\n",
        "We also have the embeddings of wikipedia links (which are themselves Wikipedia pages). We can take a similar approach to extract these and find the most similar to a query page.\n",
        "\n",
        "Let's write a quick function to extract weights from a model given the name of the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_weights(name, model):\n",
        "    \"\"\"Extract weights from a neural network model\"\"\"\n",
        "    \n",
        "    # Extract weights\n",
        "    weight_layer = model.get_layer(name)\n",
        "    weights = weight_layer.get_weights()[0]\n",
        "    \n",
        "    # Normalize\n",
        "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
        "    return weights\n",
        "\n",
        "link_weights = extract_weights('link_embedding', model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('science fiction', link_weights, index_name = 'page')\n",
        "\n",
        "find_similar('biography', link_weights, index_name = 'page')\n",
        "\n",
        "find_similar('biography', link_weights, index_name = 'page', n = 5, plot = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "find_similar('new york city', link_weights, index_name = 'page', n = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification Model\n",
        "I was curious if training for the mean squared error as a regression problem was the ideal approach, so I also decided to experiment with a classification model. For this model, the negative examples receive a label of 0 and the loss function is binary cross entropy. The procedure for the neural network to learn the embeddings is exactly the same, only it will be optimizing for a slightly different measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_class = book_embedding_model(50, classification = True)\n",
        "gen = generate_batch(pairs, n_positive, negative_ratio=2, classification = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "754/754 [==============================] - 30s 40ms/step - loss: 3.3750e-05\n",
            "Epoch 13/15\n",
            "754/754 [==============================] - 32s 42ms/step - loss: 0.0014\n",
            "Epoch 14/15\n",
            "754/754 [==============================] - 32s 42ms/step - loss: 3.3314e-05\n",
            "Epoch 15/15\n",
            "754/754 [==============================] - 33s 43ms/step - loss: 3.3681e-05\n"
          ]
        }
      ],
      "source": [
        "# Train the model to learn embeddings\n",
        "h = model.fit_generator(gen, epochs = 15, steps_per_epoch = len(pairs) // n_positive, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model_class.save('first_attempt_class.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(37020, 50)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Books closest to War and Peace.\n",
            "\n",
            "Book: War and Peace                   Similarity: 1.0\n",
            "Book: Crewel (novel)                  Similarity: 0.5\n",
            "Book: The Elusive Pimpernel (novel)   Similarity: 0.49\n",
            "Book: Magic of Eberron                Similarity: 0.49\n",
            "Book: The Dreamers (novel series)     Similarity: 0.48\n"
          ]
        }
      ],
      "source": [
        "book_weights_class = extract_weights('book_embedding', model_class)\n",
        "book_weights_class.shape\n",
        "\n",
        "find_similar('War and Peace', book_weights_class, n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Books closest to The Fellowship of the Ring.\n",
            "\n",
            "Book: The Fellowship of the Ring   Similarity: 1.0\n",
            "Book: Tales of Dunk and Egg        Similarity: 0.53\n",
            "Book: Chitta Lahu                  Similarity: 0.51\n",
            "Book: Buddy (Herlong novel)        Similarity: 0.51\n",
            "Book: Natural Symbols              Similarity: 0.51\n"
          ]
        }
      ],
      "source": [
        "find_similar('The Fellowship of the Ring', book_weights_class, n = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizations\n",
        "One of the most interesting parts about embeddings is that we can use them to visualize concepts such as War and Peace or biography. First we have to take the embeddings from 50 dimensions down to either 3 or 2. We can do this using pca, tsne, or umap. We'll try both tsne and umap for comparison. TSNE takes much longer and is designed to retain local structure within the data. UMAP is generally quicker and is designed for a balance between local and global structure in the embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from umap-learn) (1.22.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from umap-learn) (1.8.0)\n",
            "Collecting numba>=0.49\n",
            "  Downloading numba-0.55.1-cp38-cp38-win_amd64.whl (2.4 MB)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from numba>=0.49->umap-learn) (61.2.0)\n",
            "Collecting numpy>=1.17\n",
            "  Downloading numpy-1.21.6-cp38-cp38-win_amd64.whl (14.0 MB)\n",
            "Collecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp38-cp38-win_amd64.whl (23.2 MB)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\kwonk\\miniconda3\\envs\\kwon\\lib\\site-packages (from tqdm->umap-learn) (0.4.4)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py): started\n",
            "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=f877bd795b4ed4bab406dac0826b56989e0125bcd591f4f6fb13dbe46de596ae\n",
            "  Stored in directory: c:\\users\\kwonk\\appdata\\local\\pip\\cache\\wheels\\a9\\3a\\67\\06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "  Building wheel for pynndescent (setup.py): started\n",
            "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=41b5db3190429329a28bd2d3d62bd77b0fdee09f6a5e24561ceac6128975fdf7\n",
            "  Stored in directory: c:\\users\\kwonk\\appdata\\local\\pip\\cache\\wheels\\1d\\07\\6e\\9ae4e883392994fd1d7c61a0377f0177e3f8e2faff6c677341\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: numpy, llvmlite, numba, tqdm, pynndescent, umap-learn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.3\n",
            "    Uninstalling numpy-1.22.3:\n",
            "      Successfully uninstalled numpy-1.22.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'C:\\\\Users\\\\kwonk\\\\miniconda3\\\\envs\\\\kwon\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "book_r = reduce_dim(book_weights_class, components = 2, method = 'tsne')\n",
        "book_r.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NNERS.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0a0b40399c3ab138606cfd7cf7c33e76e63fdf6dc7f1f51720384aaa6fb56eca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.5 ('env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
