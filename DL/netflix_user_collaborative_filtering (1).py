# -*- coding: utf-8 -*-
"""Netflix_user_collaborative_filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mTjLDyVmCqGg0aBeY2J_KGWAjlO6PWOm
"""

###
'''Netfilx Recommendation System : User database Collaborative Filtering'''

!pip install scikit-surprise

# 라이브러리 설정
import pandas as pd
import numpy as np
import math
import re

import matplotlib.pyplot as plt
import seaborn as sns

from scipy.sparse import csr_matrix
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate # cross_validate = evaluate -> surprise 버전 변경으로 변경됨
sns.set_style("ticks")

user_data1 = pd.read_csv('./drive/MyDrive/netflix_recommendation/combined_data_1.txt')
user_data1.head(5)

# 데이터셋 가져오기

user_data1 = pd.read_csv('./drive/MyDrive/netflix_recommendation/combined_data_1.txt', header = None, names=['user_id', 'Rating'], usecols = [0,1])

user_data1['Rating'] = user_data1['Rating'].astype(float)

print('User Dataset 1 shape : {}'.format(user_data1.shape))
print('-User Dataset sample-')
print(user_data1.iloc[::300000, :])

# 데이터 시각화
# agg() : 모든 열에 함수를 매핑
data_view = user_data1.groupby('Rating')['Rating'].agg(['count'])
data_view

# movie, user, rating count
# nunique() = 고유값 구하기
movie_count = user_data1.isnull().sum()[1] # 결측값 합산

# nunique() = 총 고유값의 개수(결측값 포함 x)
user_count = user_data1['user_id'].nunique() - movie_count 

rating_count = user_data1['user_id'].count() - movie_count

print(movie_count)
print(user_count)
print(rating_count)

# 평점 분포도 시각화
ax = data_view.plot(kind='barh', legend = False, figsize = (15,10))
plt.title('Total : {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, user_count, rating_count), fontsize=20)
plt.axis('off')

for i in range(1,6):
    ax.text(data_view.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, data_view.iloc[i-1][0]*100 / data_view.sum()[0]), color = 'white', weight = 'bold')

# 데이터 전처리

data_nan = pd.DataFrame(pd.isnull(user_data1.Rating)) # 결측치
data_nan = data_nan[data_nan['Rating'] == True] # rating == True 값만 가져오기
data_nan = data_nan.reset_index()
print(data_nan)

movie_np = []
movie_id = 1

for i,j in zip(data_nan['index'][1:], data_nan['index'][:-1]): # [1:] -> 1 ~ 끝 # [:-1] -> 맨 오른쪽 값을 제외한 모든 값
    temp = np.full((1,i-j-1), movie_id)
    movie_np = np.append(movie_np, temp) # movie_np list =[]에 temp 채우기
    # print(movie_np)
    movie_id += 1


record = np.full((1,len(user_data1) - data_nan.iloc[-1, 0] - 1), movie_id)
movie_np = np.append(movie_np, record)

print('Movie numpy: {}'.format(movie_np))
print('Length: {}'.format(len(movie_np)))

# remove those Movie ID rows
df = user_data1[pd.notnull(user_data1['Rating'])]
df

df['movie_id'] = movie_np.astype(int) # 'movie_id' 컬럼 추가
df['user_id'] = df['user_id'].astype(int)
print('-Dataset examples-')
print(df.iloc[::5000000, :])

## 데이터 유효값 전처리

# 1) 리뷰가 너무 적은 영화 제거
# 2) 리뷰를 너무 적게 제공하는 고객 제거

number = ['count', 'mean']


# movie/user 최소 유효값 구하기
movie_summary = df.groupby('movie_id')['Rating'].agg(number)
movie_summary.index = movie_summary.index.map(int)


movie_mark = round(movie_summary['count'].quantile(0.9),0) # quantile 분위값 구하기
remove_movie_list = movie_summary[movie_summary['count'] < movie_mark].index

print('Movies minimum times of review: {}'.format(movie_mark))


# user 최소 유효값 구하기
user_summary = df.groupby('user_id')['Rating'].agg(number)
user_summary.index = user_summary.index.map(int)


user_mark = round(user_summary['count'].quantile(0.9),0) # quantile 분위값 구하기
remove_user_list = user_summary[user_summary['count'] < user_mark].index

print('Users minimum times of review: {}'.format(user_mark))


# 데이터 소거
# ~df = is not in(여집합)
print('Original Shape: {}'.format(df.shape))
df = df[~df['movie_id'].isin(remove_movie_list)]
df = df[~df['user_id'].isin(remove_user_list)]
print('After Trim Shape: {}'.format(df.shape))
df.head(5)

print('-Data Examples-')
print(df.iloc[::5000000, :])

## 데이터 셋 피벗테이블 정리

df_pivot = pd.pivot_table(df,values = 'Rating', index = 'user_id', columns = 'movie_id')
print(df_pivot.shape)

df_pivot

## 데이터 mapping
# movie_titles.csv 가져오기
movie_title = pd.read_csv('/content/drive/MyDrive/netflix_recommendation/movie_titles.csv', encoding = 'ISO-8859-1', header = None, names = ['movie_id', 'year', 'name'])

movie_title.set_index('movie_id', inplace = True)
movie_title
# print(movie_title.head(10))



reader = Reader()

# 10만개 데이터만 가져오기 -> 속도
# df = 유효값 전처리 데이터
m_data = Dataset.load_from_df(df[['user_id', 'movie_id', 'Rating']][:100000], reader)

# cross_val_score()

algo = SVD()
cross_validate(algo, m_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

df_svd = df[(df['movie_id'] == 4445) & (df['Rating'] == 5)]
print(df_svd)
df_svd = df_svd.set_index('movie_id')
print(df_svd)
df_svd = df_svd.join(movie_title)['name']
print(df)
print(movie_title)
print(df_svd)
# print(df_svd)

user_svd = movie_title.copy()
user_svd = user_svd.reset_index()
user_svd = user_svd[~user_svd['movie_id'].isin(remove_movie_list)]

#
user_data = Dataset.load_from_df(df[['user_id', 'movie_id', 'Rating']], reader)
trainset = user_data.build_full_trainset()
algo.fit(trainset)

print(user_svd)
print(user_data)

user_svd = movie_title.copy()
user_svd = user_svd.reset_index()
print(user_svd)

user_svd = user_svd[~user_svd['movie_id'].isin(remove_movie_list)]
print(user_svd)

algo.predict(561, 121)

algo.predict(5, 4562)

algo.predict(449391, 4488)

user_svd['Estimate_Score'] = user_svd['movie_id'].apply(lambda x: algo.predict(7000000, x).est)

# movie_id 제거
user_svd = user_svd.drop('movie_id', axis = 1)

# 예상 평저 오름차순 정렬
user_svd = user_svd.sort_values('Estimate_Score', ascending=False)
print(user_svd.head(200))

## 데이터 mapping
# movie_titles.csv 가져오기
movie_title = pd.read_csv('/content/drive/MyDrive/netflix_recommendation/movie_titles.csv', encoding = 'ISO-8859-1', header = None, names = ['movie_id', 'year', 'name'])
movie_title.set_index('movie_id', inplace = True)
movie_title.head()

# 예측 점수 추천시스템
# 분석 # ??

def recommend(title, min_count): # movie_title = title, min_count 
    print("For movie ({})".format(movie_title))
    print("- Top 10 movies recommended based on Pearsons'R correlation - ")
    i = int(movie_title.index[movie_title['name'] == title][0])
    target = df_pivot[i]
    similar_to_target = df_pivot.corrwith(target)
    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])
    corr_target.dropna(inplace = True)
    corr_target = corr_target.sort_values('PearsonR', ascending = False)
    corr_target.index = corr_target.index.map(int)
    corr_target = corr_target.join(movie_title).join(movie_summary)[['PearsonR', 'name', 'count', 'mean']]
    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))

recommend("The Pacifier", 1)

recommend("Blade: Trinity" ,0)

recommend("Batman Begins", 0)

recommend("Batman Begins", 0)

title = movie_title
min_count = 0

# movie_title.index == title -> 입력 값이 데이터에 실제로 있는가?
i = int(movie_title.index[movie_title['name'] == title][0])
# 입력값의 피벗테이블 행 가져오기
target = df_pivot[i]
# 피어슨 유사도 계산
similar_to_target = df_pivot.corrwith(target)

corr_target = pd.DataFrame(similar_to_target, columns = ['Pearson_similarity'])
corr_target.dropna(inplace = True)
corr_target = corr_target.sort_values('PearsonR', ascending = False)
corr_target.index = corr_target.index.map(int)
corr_target = corr_target.join(movie_title).join(movie_summary)[['Pearson_similarity', 'name', 'count', 'mean']]
print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))

# 예측 점수 추천시스템
# 분석 # ??

def recommend(title, min_count): # movie_title = title, min_count 
    print("For movie ({})".format(movie_title))
    print("- Top 10 movies recommended based on Pearsons'R correlation - ")
    # movie_title.index == title -> 입력 값이 데이터에 실제로 있는가?
    i = int(movie_title.index[movie_title['name'] == title][0])
    # 입력값의 피벗테이블 행 가져오기
    target = df_pivot[i]
    # 피어슨 유사도 계산
    similar_to_target = df_pivot.corrwith(target) 

    
    corr_target = pd.DataFrame(similar_to_target, columns = ['Pearson_similarity'])
    corr_target.dropna(inplace = True)
    corr_target = corr_target.sort_values('PearsonR', ascending = False)
    corr_target.index = corr_target.index.map(int)
    corr_target = corr_target.join(movie_title).join(movie_summary)[['Pearson_similarity', 'name', 'count', 'mean']]
    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))

